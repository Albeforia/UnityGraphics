// We do not rely on recursion, beyong shooting shadow and random walk rays from the intersected surface
#pragma max_recursion_depth 2

// HDRP include
#define SHADER_TARGET 50

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Sampling/Sampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/Builtin/BuiltinData.hlsl"

// Ray tracing includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/ShaderVariablesRaytracing.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/Common/AtmosphericScatteringRayTracing.hlsl"

// We need this for the potential volumetric integration on camera misses
#define HAS_LIGHTLOOP

// Path tracing includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingPayload.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingSampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingSkySampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingVolume.hlsl"

// Input(s)
TEXTURE2D_X(_SkyCameraTexture);
float4x4    _PixelCoordToViewDirWS;
int         _PathTracingCameraSkyEnabled;
float4      _PathTracingCameraClearColor;
float4      _PathTracingDoFParameters;    // x: aperture radius, y: focus distance, zw: unused
float4      _PathTracingTilingParameters; // xy: tile count, zw: current tile index

// Output(s)
RW_TEXTURE2D_X(float4, _FrameTexture);

float4 GetCameraMissValue(uint2 xy)
{
    return IsSkyEnabled() && _PathTracingCameraSkyEnabled ?
        _SkyCameraTexture[COORD_TEXTURE2D_X(xy)] : _PathTracingCameraClearColor * GetInverseCurrentExposureMultiplier();
}

float3 GetMaterialMissValue()
{
    return IsSkyEnabled() ? GetSkyValue(WorldRayDirection()) : 0.0;
}

[shader("miss")]
void MissCamera(inout PathPayload payload : SV_RayPayload)
{
    // Set initial "hit" to infinity (before potential volumetric scattering)
    payload.rayTHit = FLT_INF;

    // In indirect-only mode, it makes more sense to return a null value
    if (_RaytracingMinRecursion > 1)
    {
        payload.value = 0.0;
        payload.throughput.x = 0.0;
        return;
    }

    float4 missColor = GetCameraMissValue(payload.pixelCoord);

    payload.value = missColor.rgb;
    payload.throughput.x = missColor.a;

    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value, payload.throughput.x);

    if (_EnableVolumetricFog)
    {
        float3 lightPosition, envValue = payload.value;

        // Generate a 4D unit-square sample for this depth, from our QMC sequence
        float4 inputSample = GetSample4D(payload.pixelCoord, _RaytracingSampleIndex, 0);

        // Compute volumetric scattering
        payload.value = 0.0;
        float pdf = 1.0;
        bool sampleLocalLights;
        if (SampleVolumeScatteringPosition(payload.pixelCoord, inputSample.w, payload.rayTHit, pdf, sampleLocalLights, lightPosition))
        {
            ComputeVolumeScattering(payload, inputSample.xyz, sampleLocalLights, lightPosition);

            // Apply the pdf
            payload.value /= pdf;

            // Apply volumetric attenuation
            ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.rayTHit, payload.value, false);
        }

        // Reinject the environment value
        payload.value += envValue;
    }
}

[shader("miss")]
void MissLight(inout PathPayload payload : SV_RayPayload)
{
}

[shader("miss")]
void MissMaterial(inout PathPayload payload : SV_RayPayload)
{
    if (IsSkySamplingEnabled() || payload.segmentID < _RaytracingMinRecursion)
    {
        payload.value = 0.0;
        return;
    }

    payload.value = GetMaterialMissValue();
    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value);

    // Make sure we don't shoot another continuation ray after this
    payload.rayDirection = 0.0;
}

[shader("miss")]
void MissTransparentUnlitMaterial(inout PathPayload payload : SV_RayPayload)
{
    if (payload.segmentID < _RaytracingMinRecursion)
    {
        payload.value = 0.0;
        return;
    }

    // Misses from Transparent Unlit are special in that they always are straight up continuation rays
    payload.value = payload.segmentID > 1 ? GetMaterialMissValue() : GetCameraMissValue(payload.pixelCoord).rgb;
    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value);

    // Make sure we don't shoot another continuation ray after this
    payload.rayDirection = 0.0;
}

uint GetMissIndex(RayDesc ray)
{
    // If we have specific Min and Max values, we are shooting a Transparent Unlit continuation ray
    return (ray.TMin == 0.0 && ray.TMax == FLT_INF) ? 3 : 2;
}

void ApplyDepthOfField(uint2 pixelCoord, float dotDirection, inout float3 origin, inout float3 direction)
{
     // Check aperture radius
    if (_PathTracingDoFParameters.x <= 0.0)
        return;

    // Sample the lens aperture using the next available dimensions
    // (we use 40 for path tracing, 2 for sub-pixel jittering, 64 for SSS -> 106, 107)
    float2 uv = _PathTracingDoFParameters.x * SampleDiskUniform(GetSample(pixelCoord, _RaytracingSampleIndex, 106),
                                                                GetSample(pixelCoord, _RaytracingSampleIndex, 107));

    // Compute the focus point by intersecting the pinhole ray with the focus plane
    float t = _PathTracingDoFParameters.y / dotDirection;
    float3 focusPoint = origin + t * direction;

    // Compute the new ray origin (_ViewMatrix[0] = right, _ViewMatrix[1] = up)
    origin += _ViewMatrix[0].xyz * uv.x + _ViewMatrix[1].xyz * uv.y;

    // The new ray direction should pass through the focus point
    direction = normalize(focusPoint - origin);
}

void GenerateCameraRay(uint2 pixelCoord, out PathPayload payload, out RayDesc ray)
{
    // Get the current tile coordinates (for interleaved tiling) and update pixel coordinates accordingly
    uint2 tileCount = uint2(_PathTracingTilingParameters.xy);
    uint2 tileIndex = uint2(_PathTracingTilingParameters.zw);
    uint2 tiledPixelCoord = pixelCoord * tileCount + tileIndex;

    // Jitter them (we use 4x10 dimensions of our sequence during path tracing atm, so pick the next available ones)
    float4 jitteredPixelCoord = float4(pixelCoord, 1.0, 1.0);
    jitteredPixelCoord.x += GetSample(tiledPixelCoord, _RaytracingSampleIndex, 40) / tileCount.x;
    jitteredPixelCoord.y += GetSample(tiledPixelCoord, _RaytracingSampleIndex, 41) / tileCount.y;

    // Initialize the payload for this camera ray
    payload.throughput = 1.0;
    payload.value = 0.0;
    payload.maxRoughness = 0.0;
    payload.pixelCoord = tiledPixelCoord;
    payload.segmentID = 0;

    // In order to achieve texture filtering, we need to compute the spread angle of the subpixel
    payload.cone.spreadAngle = _RaytracingPixelSpreadAngle / min(tileCount.x, tileCount.y);
    payload.cone.width = 0.0;

    // Generate the ray descriptor for this pixel
    ray.TMin = _RaytracingCameraNearPlane;
    ray.TMax = FLT_INF;

    // We need the camera forward direction in both types of projection
    float3 cameraDirection = GetViewForwardDir();

    // Compute the ray's origin and direction, for either perspective or orthographic projection
    if (IsPerspectiveProjection())
    {
        ray.Origin = GetPrimaryCameraPosition();
        ray.Direction = -normalize(mul(jitteredPixelCoord, _PixelCoordToViewDirWS).xyz);

        // Use planar clipping, to match rasterization
        float dotDirection = dot(cameraDirection, ray.Direction);
        ray.TMin /= dotDirection;

        ApplyDepthOfField(tiledPixelCoord, dotDirection, ray.Origin, ray.Direction);
    }
    else // Orthographic projection
    {
        uint2 pixelResolution = DispatchRaysDimensions().xy;
        float4 screenCoord = float4(2.0 * jitteredPixelCoord.x / pixelResolution.x - 1.0,
                                    -2.0 * jitteredPixelCoord.y / pixelResolution.y + 1.0,
                                    0.0, 0.0);

        ray.Origin = mul(_InvViewProjMatrix, screenCoord).xyz;
        ray.Direction = cameraDirection;
    }
}

float3 ClampValue(float3 value)
{
    float intensity = Luminance(value) * GetCurrentExposureMultiplier();
    return intensity > _RaytracingIntensityClamp ? value * _RaytracingIntensityClamp / intensity : value;
}

[shader("raygeneration")]
void TracePath()
{
    // Get the current integer pixel coordinates
    uint2 pixelCoord = DispatchRaysIndex().xy;

    // Generate the camera segment of our path
    PathPayload payload;
    RayDesc ray;
    GenerateCameraRay(pixelCoord, payload, ray);

    // Trace our camera ray
    TraceRay(_RaytracingAccelerationStructure, RAY_FLAG_CULL_BACK_FACING_TRIANGLES, RAYTRACINGRENDERERFLAG_PATH_TRACING, 0, 1, 0, ray, payload);

    // These are the quantities we want to compute for the path
    float3 value = payload.value;
    float alpha;

    // Test if something was hit from our camera ray
    if (payload.rayTHit < FLT_INF)
    {
        // If we hit something, always set alpha/presence to 1.0
        alpha = 1.0;

        // To apply our battle-tested, recursive clamping strategy, we will store results at each depth
        uint segmentID, segmentIDMaxDepth = min(_RaytracingMaxRecursion, SEGMENT_ID_MAX_DEPTH);
        float3 segmentValue[SEGMENT_ID_MAX_DEPTH];
        float3 segmentThroughput[SEGMENT_ID_MAX_DEPTH];

        segmentValue[0] = value;
        segmentThroughput[0] = 1.0;

        // Iterate over each path segments, up to a specified max recursion depth
        for (segmentID = 1; segmentID < segmentIDMaxDepth && any(payload.rayDirection); segmentID++)
        {
            // Store throughput up to that segment
            segmentThroughput[segmentID] = payload.throughput;

            // Update our continuation ray
            GetContinuationRay(payload, ray);

            // Refresh the payload (done after extracting continuation ray, because of data aliasing)
            payload.pixelCoord = pixelCoord;
            payload.segmentID = segmentID;

            // Trace our continuation ray
            TraceRay(_RaytracingAccelerationStructure, RAY_FLAG_CULL_BACK_FACING_TRIANGLES,
                     RAYTRACINGRENDERERFLAG_PATH_TRACING, 0, 1, GetMissIndex(ray), ray, payload);

            // Store segment radiance value
            segmentValue[segmentID] = payload.value;
        }

        // Then iterate backwards, to apply our recursive intensity clamping
        for (segmentID--; segmentID > 0; segmentID--)
        {
            segmentThroughput[segmentID] /= segmentThroughput[segmentID - 1];
            segmentValue[segmentID - 1] += segmentThroughput[segmentID] * ClampValue(segmentValue[segmentID]);
        }
        value = segmentValue[0];
    }
    else
    {
        // Use the alpha of the background/sky
        alpha = payload.throughput.x;
    }

    // Copy the full path radiance value (and alpha/presence) to our output buffer
    _FrameTexture[COORD_TEXTURE2D_X(pixelCoord)] = float4(value, alpha);
}
