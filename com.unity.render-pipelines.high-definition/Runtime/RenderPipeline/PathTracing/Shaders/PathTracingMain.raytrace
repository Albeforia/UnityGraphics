// We do not rely on recursion, beyong shooting shadow and random walk rays from the intersected surface
#pragma max_recursion_depth 2

// HDRP include
#define SHADER_TARGET 50

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Sampling/Sampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/Builtin/BuiltinData.hlsl"

// Ray tracing includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/ShaderVariablesRaytracing.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/Common/AtmosphericScatteringRayTracing.hlsl"

// We need this for the potential volumetric integration on camera misses
#define HAS_LIGHTLOOP

// Path tracing includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingIntersection.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingSampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingSkySampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingVolume.hlsl"

// Input(s)
TEXTURE2D_X(_SkyCameraTexture);
float4x4    _PixelCoordToViewDirWS;
int         _PathTracingCameraSkyEnabled;
float4      _PathTracingCameraClearColor;
float4      _PathTracingDoFParameters;    // x: aperture radius, y: focus distance, zw: unused
float4      _PathTracingTilingParameters; // xy: tile count, zw: current tile index

// Output(s)
RW_TEXTURE2D_X(float4, _FrameTexture);

float4 GetCameraMissValue(uint2 xy)
{
    return IsSkyEnabled() && _PathTracingCameraSkyEnabled ?
        _SkyCameraTexture[COORD_TEXTURE2D_X(xy)] : _PathTracingCameraClearColor * GetInverseCurrentExposureMultiplier();
}

float3 GetMaterialMissValue()
{
    return IsSkyEnabled() ? GetSkyValue(WorldRayDirection()) : 0.0;
}

[shader("miss")]
void MissCamera(inout PathIntersection payload : SV_RayPayload)
{
    payload.t = FLT_INF;

    // In indirect-only mode, it makes more sense to return a null value
    if (_RaytracingMinRecursion > 1)
    {
        payload.value = 0.0;
        payload.throughput.x = 0.0;
        return;
    }

    float4 missColor = GetCameraMissValue(payload.pixelCoord);

    payload.value = missColor.rgb;
    payload.throughput.x = missColor.a;

    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value, payload.throughput.x);

    if (_EnableVolumetricFog)
    {
        float3 lightPosition, envValue = payload.value;

        // Generate a 4D unit-square sample for this depth, from our QMC sequence
        float4 inputSample = GetSample4D(payload.pixelCoord, _RaytracingSampleIndex, 0);

        // Compute volumetric scattering
        payload.value = 0.0;
        float pdf = 1.0;
        bool sampleLocalLights;
        if (SampleVolumeScatteringPosition(payload.pixelCoord, inputSample.w, payload.t, pdf, sampleLocalLights, lightPosition))
        {
            ComputeVolumeScattering(payload, inputSample.xyz, sampleLocalLights, lightPosition);

            // Apply the pdf
            payload.value /= pdf;

            // Apply volumetric attenuation
            ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.t, payload.value, false);
        }

        // Reinject the environment value
        payload.value += envValue;
    }
}

[shader("miss")]
void MissLight(inout PathIntersection payload : SV_RayPayload)
{
}

[shader("miss")]
void MissMaterial(inout PathIntersection payload : SV_RayPayload)
{
    if (IsSkySamplingEnabled() || GetCurrentDepth(payload) < _RaytracingMinRecursion)
    {
        payload.value = 0.0;
        return;
    }

    payload.value = GetMaterialMissValue();
    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value);
    payload.value *= payload.throughput;

    // Make sure we don't shoot a continuation ray after this
    payload.ray.Direction = 0.0;
}

[shader("miss")]
void MissTransparentUnlitMaterial(inout PathIntersection payload : SV_RayPayload)
{
    int currentDepth = GetCurrentDepth(payload);

    if (currentDepth < _RaytracingMinRecursion)
    {
        payload.value = 0.0;
        return;
    }

    // Misses from Transparent Unlit are special in that they always are straight up continuation rays
    payload.value = currentDepth > 1 ? GetMaterialMissValue() : GetCameraMissValue(payload.pixelCoord).rgb;
    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value);
}

void ApplyDepthOfField(uint2 pixelCoord, float dotDirection, inout float3 origin, inout float3 direction)
{
     // Check aperture radius
    if (_PathTracingDoFParameters.x <= 0.0)
        return;

    // Sample the lens aperture using the next available dimensions
    // (we use 40 for path tracing, 2 for sub-pixel jittering, 64 for SSS -> 106, 107)
    float2 uv = _PathTracingDoFParameters.x * SampleDiskUniform(GetSample(pixelCoord, _RaytracingSampleIndex, 106),
                                                                GetSample(pixelCoord, _RaytracingSampleIndex, 107));

    // Compute the focus point by intersecting the pinhole ray with the focus plane
    float t = _PathTracingDoFParameters.y / dotDirection;
    float3 focusPoint = origin + t * direction;

    // Compute the new ray origin (_ViewMatrix[0] = right, _ViewMatrix[1] = up)
    origin += _ViewMatrix[0].xyz * uv.x + _ViewMatrix[1].xyz * uv.y;

    // The new ray direction should pass through the focus point
    direction = normalize(focusPoint - origin);
}

void GenerateCameraRay(uint2 pixelCoord, out PathIntersection payload)
{
    // Get the current tile coordinates (for interleaved tiling) and update pixel coordinates accordingly
    uint2 tileCount = uint2(_PathTracingTilingParameters.xy);
    uint2 tileIndex = uint2(_PathTracingTilingParameters.zw);
    uint2 tiledPixelCoord = pixelCoord * tileCount + tileIndex;

    // Jitter them (we use 4x10 dimensions of our sequence during path tracing atm, so pick the next available ones)
    float4 jitteredPixelCoord = float4(pixelCoord, 1.0, 1.0);
    jitteredPixelCoord.x += GetSample(tiledPixelCoord, _RaytracingSampleIndex, 40) / tileCount.x;
    jitteredPixelCoord.y += GetSample(tiledPixelCoord, _RaytracingSampleIndex, 41) / tileCount.y;

    // Initialize the payload for this camera ray
    payload.throughput = 1.0;
    payload.value = 0.0;
    payload.remainingDepth = _RaytracingMaxRecursion;
    payload.pixelCoord = tiledPixelCoord;
    payload.maxRoughness = 0.0;

    // In order to achieve texture filtering, we need to compute the spread angle of the subpixel
    payload.cone.spreadAngle = _RaytracingPixelSpreadAngle / min(tileCount.x, tileCount.y);
    payload.cone.width = 0.0;

    // Generate the ray descriptor for this pixel
    payload.ray.TMin = _RaytracingCameraNearPlane;
    payload.ray.TMax = FLT_INF;

    // We need the camera forward direction in both types of projection
    float3 cameraDirection = GetViewForwardDir();

    // Compute the ray's origin and direction, for either perspective or orthographic projection
    if (IsPerspectiveProjection())
    {
        payload.ray.Origin = GetPrimaryCameraPosition();
        payload.ray.Direction = -normalize(mul(jitteredPixelCoord, _PixelCoordToViewDirWS).xyz);

        // Use planar clipping, to match rasterization
        float dotDirection = dot(cameraDirection, payload.ray.Direction);
        payload.ray.TMin /= dotDirection;

        ApplyDepthOfField(tiledPixelCoord, dotDirection, payload.ray.Origin, payload.ray.Direction);
    }
    else // Orthographic projection
    {
        uint2 pixelResolution = DispatchRaysDimensions().xy;
        float4 screenCoord = float4(2.0 * jitteredPixelCoord.x / pixelResolution.x - 1.0,
                                    -2.0 * jitteredPixelCoord.y / pixelResolution.y + 1.0,
                                    0.0, 0.0);

        payload.ray.Origin = mul(_InvViewProjMatrix, screenCoord).xyz;
        payload.ray.Direction = cameraDirection;
    }
}

[shader("raygeneration")]
void TracePath()
{
    // Get the current integer pixel coordinates
    uint2 pixelCoord = DispatchRaysIndex().xy;

    // Generate the camera segment of our path
    PathIntersection payload;
    GenerateCameraRay(pixelCoord, payload);

    // Trace our camera ray
    TraceRay(_RaytracingAccelerationStructure, RAY_FLAG_CULL_BACK_FACING_TRIANGLES, RAYTRACINGRENDERERFLAG_PATH_TRACING, 0, 1, 0, payload.ray, payload);

    // These are the quantities we want to compute for the path
    float3 value = payload.value;
    float alpha;

    // Test if something was hit from our camera ray
    if (payload.t < FLT_INF)
    {
        // If we hit something, always set alpha/presence to 1.0
        alpha = 1.0;

        // Iterate over each path segments, up to a specified max recursion value
        for (uint iter = 1; iter < _RaytracingMaxRecursion && any(payload.ray.Direction); iter++)
        {
            // Update the payload
            //payload.pixelCoord = pixelCoord;
            payload.remainingDepth = _RaytracingMaxRecursion - iter;

            // Trace our continuation ray
            TraceRay(_RaytracingAccelerationStructure, RAY_FLAG_CULL_BACK_FACING_TRIANGLES, RAYTRACINGRENDERERFLAG_PATH_TRACING, 0, 1, 2, payload.ray, payload);

            // Add the value for this path segment
            value += payload.value;
        }
    }
    else
    {
        // Use the alpha of the background/sky
        alpha = payload.throughput.x;
    }

    // Copy the full path radiance value (and alpha/presence) to our output buffer
    _FrameTexture[COORD_TEXTURE2D_X(pixelCoord)] = float4(value, alpha);
}
